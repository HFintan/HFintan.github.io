
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Research: Compressed Sensing</title>

<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Fintan Hegarty</title>
	<link href="Style.css" rel="stylesheet" media="screen" />
	<script type="text/JavaScript" src="curvycorners.js"></script>
</head>

<body>
<div id="container">
<div id="wrapper">
<div id="banner">
<h1>Fintan Hegarty</h1>
<p>Research: Compressed Sensing<br />
<p><br />
</p>
</div>
<div id="nav">
<ul>
<li><a href="index.html"><big>Home</big></a></li>
<li><a href="research.html"><big>Research</big></a></li>
<li><a href="teaching.html"><big>Teaching</big></a></li>
<!--<li><a href="stop.html"><big> Eile </big></a></li>-->
<!--<li><a href="index.html"><big>English version</big></a></li>-->
</ul>
</div>   
<div id="page">
<div id="content">

The idea behind compressed sensing concerns the compression of a vector x into a smaller vector y=Ax via an m x n matrix A where
<br> m << n , and the subsequent recovery of x from y and A. This system is underdetermined - there are infinitely many solutions
<br> to y=Ax. However, for k-sparse vectors x (ie. x contains at most k non-zero entries) it turns out that we can solve y=Ax
<br> if A has certain properties, as detailed by Candés, Donoho, and Tao.
<br>
<br><b>Definition:</b> If &#934; and &#934;' are matrices such that &#934;'<sub>i,j</sub>=&#934;<sub>i,j</sub> for every non-zero entry
of &#934;', then we say that &#934;' is a <i>sparsification</i> of &#934;.<br>
The density of &#934;, &#948;(&#934;) is the proportion of non-zero entries which &#934; contains.<br> 
<br>
In my paper with Ó Catháin and Zhao, we analyse the effect of the sparsification of the sensing matrix &#934; on its recovery capabilities.
<br>
The graph below is a motivational figure from our paper. It shows that the sparsified matrices (&delta;=0.05 and 0.1) outperform the original matrix (&delta;=1). <br>

<img src="https://cloud.githubusercontent.com/assets/4904401/6910681/45d76454-d74e-11e4-92a7-4b574759d38c.png" height="300"/><br>

This graph was generated using this <a href="MakingFig1.m">MakingFig1 Matlab file</a> and the data in this <a href="Fig1Data.txt"> Fig1Data .txt file</a>. <br>  

If you wish to run similar experiments yourself, the Matlab files used to generate the above data are:<br>
<li><a href="sparsitySuccessTest.m">Sparsity Success Test</a></li>
<li><a href="testOneMat.m">Test One Mat</a></li>
<li><a href="sparsifyMat.m">Sparsify Mat</a></li>
<li><a href="randomKSparseVector.m">Random k-sparse vector</a></li>
<li><a href="generateMat.m">Generate Mat</a></li>
<li><a href="sparsifyingMat.m">Sparsifying Mat</a></li>
<li><a href="generateSparsitySuccessTestGraph.m">Generate Sparsity Success Test Graph</a></li>

and the parameters were
<b>generateSparsitySuccessTestGraph(200,2000,'N',[1,0.1,0.05],1,60,500);</b> where
<ul>
<li>200x2000 are the dimensions of the matrices we consider</li>
<li>'N' indicates that the entry values in the matrices are normally distributed (though we then take the absolute value)</li> 
<li>[1,0.1,0.05] are the sparsification levels we test - 1 being the original matrix and 0.05 containing 95% zero entries</li>
<li>1 and 60 are the lower and upper bounds on the sparsity of the signal</li>
<li>500 is the number of iterations over each matrix sparsity level</li>
(Data is sent to sparsitySuccessTestGraphData.txt by default)
</ul>

<br><br><br>
<h2>Speed</h2>

In the two graphs below, we examine the effect of sparsification on the run-time of the algorithm. For a range of vector sparsities,
we timed 100 recovery attempts from 200x2000 sensing matrices, using both the standard linear programming solver in Matlab, and the 
specialised CoSaMP algorithm developed by Needell and Tropp.
We see that CoSaMP is significantly faster than the linear programming solver, and that the sparsified matrices recover noticeably
more vectors than the unsparsified ones. The unsparsified CoSaMP is slightly faster in some places - accounted for by the fraction
of a second it takes to sparsify a matrix, but the enormous improvement in recovery shows that sparsification is worthwhile.
<img border="”"-100" src="https://cloud.githubusercontent.com/assets/4904401/7548132/17c52d4c-f5f5-11e4-882b-24c5c335aa86.png" width="900" />
<img border="-100" src="https://cloud.githubusercontent.com/assets/4904401/7548227/396592a8-f5f9-11e4-8b0a-0435198aaa9b.png" width="900" />

<br><br><br>
Update test

<!--<table><tbody>
<!--<tr><td><img border=”5″ src="https://cloud.githubusercontent.com/assets/4904401/7548132/17c52d4c-f5f5-11e4-882b-24c5c335aa86.png" height="300" />-->
<!--</td><td><img border=”5″ src="https://cloud.githubusercontent.com/assets/4904401/7548174/4755ca1a-f5f7-11e4-9f9a-4ccc93b69acd.png" height="300" />-->
<!--</td></tr></tbody></table>-->

<!--<img src="https://cloud.githubusercontent.com/assets/4904401/6500414/549da1b0-c303-11e4-979b-6977018992f2.png" height="300"/>
<br>
<style type="text/css">
	table.tableizer-table {
	border: 1px solid #CCC; font-family: Arial, Helvetica, sans-serif;
	font-size: 12px;
} 
.tableizer-table td {
	padding: 4px;
	margin: 3px;
	border: 1px solid #ccc;
}
.tableizer-table th {
	background-color: #104E8B; 
	color: #FFF;
	font-weight: bold;
}
-->
</body></html> 

