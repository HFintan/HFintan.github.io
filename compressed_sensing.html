
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Research: Compressed Sensing</title>

<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Fintan Hegarty</title>
	<link href="Style.css" rel="stylesheet" media="screen" />
	<script type="text/JavaScript" src="curvycorners.js"></script>
</head>

<body>
<div id="container">
<div id="wrapper">
<div id="banner">
<h1>Fintan Hegarty</h1>
<p>Research: Compressed Sensing<br />
<p><br />
</p>
</div>
<div id="nav">
<ul>
<li><a href="index.html"><big>Home</big></a></li>
<li><a href="research.html"><big>Research</big></a></li>
<li><a href="teaching.html"><big>Teaching</big></a></li>
<!--<li><a href="stop.html"><big> Eile </big></a></li>-->
<!--<li><a href="index.html"><big>English version</big></a></li>-->
</ul>
</div>   
<div id="page">
<div id="content">

The central problems in compressed sensing can be framed in terms of linear algebra. In
this model, a signal is a vector <i>v</i> in some high-dimensional vector space, <i>R<sup>N</sup></i>. The sampling
process can be described as multiplication by a specially chosen <i>n * N</i> matrix <i>&#934;</i>, called the sensing
matrix. Typically we will have <i>n << N</i>, so that the problem of recovering <i>v</i> from <i>&#934;v</i> is massively
under-determined.<br>
However, for <i>k</i>-sparse vectors <i>v</i> (ie. <i>v</i> contains at most <i>k</i> non-zero entries) it turns out that we can recover
<i>y=&#934;v<br></i> if <i>&#934;</i> has certain properties, as detailed by Candés, Donoho, and Tao.<br>

<br><b>Definition:</b> If &#934; and &#934;' are matrices such that &#934;'<sub>i,j</sub>=&#934;<sub>i,j</sub> for every non-zero entry
of &#934;', then we say that &#934;' is a <i>sparsification</i> of &#934;.<br>
The density of &#934;, &#948;(&#934;) is the proportion of non-zero entries which &#934; contains.<br> 
<br>
In my paper with Ó Catháin and Zhao, <a href="http://arxiv.org/abs/1506.03523">Sparsification of Matrices and Compressed Sensing</a>, we analyse the effect of the sparsification of the sensing matrix &#934; on its recovery capabilities.
<br>
The graph below is similar to the motivational figure from our paper. It shows that the sparsified matrices (&delta;=0.05 and 0.1) outperform the original matrix (&delta;=1). <br>
We also include the interesting section of the table which was used to generate the graph. The bolded entries indicate where at most 2% of the 500 recovery attempts failed.
<br>
<img src="https://cloud.githubusercontent.com/assets/4904401/6910681/45d76454-d74e-11e4-92a7-4b574759d38c.png" height="400" width="600"/><br>
<br><br>
<TABLE BORDER="3" CELLSPACING="1" CELLPADDING="1">
	<CAPTION>Signal Recovery Comparison of Sp(\Phi,1),Sp(\Phi,0.1),Sp(\Phi,0.05)</CAPTION>
	<TR><TD ALIGN = "center"><b>k-sparsity</b></TD>
	<TD ALIGN = "center"><b>&#948;=1</b></TD>
	<TD ALIGN = "center"><b>&#948;=0.1</b></TD>
	<TD ALIGN = "center"><b>&#948;=0.05</b></TD></TR>
	
	<TR><TD>37</TD><TD><b>499</b></TD><TD><b>500</b></TD><TD><b>499</b></TD></TR>
	<TR><TD>38</TD><TD><b>498</b></TD><TD><b>500</b></TD><TD><b>499</b></TD></TR>
	<TR><TD>39</TD><TD><b>498</b></TD><TD><b>499</b></TD><TD><b>497</b></TD></TR>
	<TR><TD>40</TD><TD>486</TD><TD><b>500</b></TD><TD><b>499</b></TD></TR>
	<TR><TD>41</TD><TD>479</TD><TD><b>498</b></TD><TD><b>500</b></TD></TR>
	<TR><TD>42</TD><TD>469</TD><TD><b>493</b></TD><TD><b>499</b></TD></TR>
	<TR><TD>43</TD><TD>449</TD><TD>489</TD><TD><b>498</b></TD></TR>
	<TR><TD>44</TD><TD>422</TD><TD>487</TD><TD><b>496</b></TD></TR>
	<TR><TD>45</TD><TD>385<TD>472</TD><TD><b>495</b></TD></TR>
	<TR><TD>46</TD><TD>359</TD><TD>451</TD><TD>486</TD></TR>
	<TR><TD>47</TD><TD>323</TD><TD>419</TD><TD>481</TD></TR>
	<TR><TD>48</TD><TD>274</TD><TD>394</TD><TD>469</TD></TR>
	<TR><TD>49</TD><TD>237</TD><TD>338</TD><TD>444</TD></TR>
	<TR><TD>50</TD><TD>186</TD><TD>308</TD><TD>400</TD></TR>
	<TR><TD>51</TD><TD>134</TD><TD>262</TD><TD>382</TD></TR>
	<TR><TD>52</TD><TD>104</TD><TD>207</TD><TD>345</TD></TR>
	<TR><TD>53</TD><TD>76</TD><TD>151</TD><TD>293</TD></TR>
	<TR><TD>54</TD><TD>48</TD><TD>126</TD><TD>245</TD></TR>
	<TR><TD>55</TD><TD>34</TD><TD>91</TD><TD>173</TD></TR>
	<TR><TD>56</TD><TD>24</TD><TD>67</TD><TD>160</TD></TR>
	<TR><TD>57</TD><TD>13</TD><TD>47</TD><TD>97</TD></TR>
	<TR><TD>58</TD><TD>7</TD><TD>28</TD><TD>80</TD></TR>
	<TR><TD>59</TD><TD>5</TD><TD>30</TD><TD>58</TD></TR>
	<TR><TD>60</TD><TD>2</TD><TD>9</TD><TD>35</TD></TR>
</TABLE>


The Matlab files used to generate the relevant data and these graphs can be found <a href="http://www.fintanhegarty.com/matlab_files">here</a>, along with further information on
parameters etc.
<br><br><br>

<h3>Algorithms</h3>
We considered three different algorithms in the paper - the standard linear programming solver in Matlab, <a href="http://uk.mathworks.com/help/optim/ug/linprog.html">linprog</a>,; 
Orthogonal Matching Pursuit (OMP) - see <a href="http://users.cms.caltech.edu/~jtropp/papers/TG07-Signal-Recovery.pdf">Tropp and Gilbert's paper</a>,
and <a href="http://arxiv.org/pdf/0803.2392v2.pdf">Needell and Tropp's</a> CoSaMP.
<br>
Both OMP and CoSaMP take a priori knowledge of the sparsity of the vector which is to be recovered, and are significantly faster
than linprog. All three, however, exhibit some degree of improvement in recovery for sparsified sensing matrices.
Figure 2 illustrates this. 

<img border="”"-100" src="https://cloud.githubusercontent.com/assets/4904401/7741978/c2fe0c40-ff7f-11e4-883b-982dd3b5ef59.png" height="400" width="800" />

<TABLE BORDER="3" CELLSPACING="1" CELLPADDING="1">
	<CAPTION>Signal Recovery Comparison of LP,CoSaMP and OMP for varying levels of sparsity</CAPTION>
	<TR><TD ALIGN = "center"><b>Algorithm</b></TD>
	<TD ALIGN = "center"><b>&#948;=1</b></TD>
	<TD ALIGN = "center"><b>&#948;=0.9</b></TD>
	<TD ALIGN = "center"><b>&#948;=0.8</b></TD>
	<TD ALIGN = "center"><b>&#948;=0.7</b></TD>
	<TD ALIGN = "center"><b>&#948;=0.6</b></TD>
	<TD ALIGN = "center"><b>&#948;=0.5</b></TD>
	<TD ALIGN = "center"><b>&#948;=0.4</b></TD>
	<TD ALIGN = "center"><b>&#948;=0.3</b></TD>
	<TD ALIGN = "center"><b>&#948;=0.2</b></TD>
	<TD ALIGN = "center"><b>&#948;=0.1</b></TD>
	<TD ALIGN = "center"><b>&#948;=0.05</b></TD>
	<TD ALIGN = "center"><b>&#948;=0.0.03</b></TD>
	<TD ALIGN = "center"><b>&#948;=0.0.02</b></TD>
	<TD ALIGN = "center"><b>&#948;=0.01</b></TD></TR>
	
	<TR><TD ALIGN = "center"><b>LP</b></TD>
	<TD ALIGN = "center">40</TD>
	<TD ALIGN = "center">40</TD>
	<TD ALIGN = "center">40</TD>
	<TD ALIGN = "center">40</TD>
	<TD ALIGN = "center">40</TD>
	<TD ALIGN = "center">40</TD>
	<TD ALIGN = "center">40</TD>
	<TD ALIGN = "center">41</TD>
	<TD ALIGN = "center">40</TD>
	<TD ALIGN = "center">43</TD>
	<TD ALIGN = "center">46</TD>
	<TD ALIGN = "center">45</TD>
	<TD ALIGN = "center">37</TD>
	<TD ALIGN = "center">3</TD></TR>

	<TR><TD ALIGN = "center"><b>OMP</b></TD>
	<TD ALIGN = "center">6</TD>
	<TD ALIGN = "center">8</TD>
	<TD ALIGN = "center">8</TD>
	<TD ALIGN = "center">9</TD>
	<TD ALIGN = "center">11</TD>
	<TD ALIGN = "center">12</TD>
	<TD ALIGN = "center">13</TD>
	<TD ALIGN = "center">15</TD>
	<TD ALIGN = "center">17</TD>
	<TD ALIGN = "center">16</TD>
	<TD ALIGN = "center">11</TD>
	<TD ALIGN = "center">5</TD>
	<TD ALIGN = "center">2</TD>
	<TD ALIGN = "center">1</TD></TR>

	<TR><TD ALIGN = "center"><b>CoSaMP</b></TD>
	<TD ALIGN = "center">27</TD>
	<TD ALIGN = "center">30</TD>
	<TD ALIGN = "center">35</TD>
	<TD ALIGN = "center">37</TD>
	<TD ALIGN = "center">37</TD>
	<TD ALIGN = "center">40</TD>
	<TD ALIGN = "center">45</TD>
	<TD ALIGN = "center">47</TD>
	<TD ALIGN = "center">50</TD>
	<TD ALIGN = "center">52</TD>
	<TD ALIGN = "center">49</TD>
	<TD ALIGN = "center">25</TD>
	<TD ALIGN = "center">1</TD>
	<TD ALIGN = "center">1</TD></TR>

</TABLE>
	


<br><br><br>
<h3>Speed</h3>

In the two graphs below, we examine the effect of sparsification on the run-time of the algorithm. For a range of vector sparsities,
we timed 100 recovery attempts from 100x1000 sensing matrices, using both the standard linear programming solver in Matlab, and the 
specialised CoSaMP algorithm developed by Needell and Tropp.
We see that CoSaMP is significantly faster than the linear programming solver, and that the sparsified matrices recover noticeably
more vectors than the unsparsified ones.<br>
These timing computations were run on an Intel(R) Core(TM) i5-3570 @ 3.4GHz with 8GB of RAM.

<img border="”"-100" src="https://cloud.githubusercontent.com/assets/4904401/10481697/59f19f10-726a-11e5-9787-be024d46a646.png" width="900" />
<img border="-100" src="https://cloud.githubusercontent.com/assets/4904401/10481744/a51e03c0-726a-11e5-8298-3a78ae363c48.png" width="900" />

<br><br><br>

<!--<table><tbody>
<!--<tr><td><img border=”5″ src="https://cloud.githubusercontent.com/assets/4904401/7548132/17c52d4c-f5f5-11e4-882b-24c5c335aa86.png" height="300" />-->
<!--</td><td><img border=”5″ src="https://cloud.githubusercontent.com/assets/4904401/7548174/4755ca1a-f5f7-11e4-9f9a-4ccc93b69acd.png" height="300" />-->
<!--</td></tr></tbody></table>-->

<!--<img src="https://cloud.githubusercontent.com/assets/4904401/6500414/549da1b0-c303-11e4-979b-6977018992f2.png" height="300"/>
<br>
<style type="text/css">
	table.tableizer-table {
	border: 1px solid #CCC; font-family: Arial, Helvetica, sans-serif;
	font-size: 12px;
} 
.tableizer-table td {
	padding: 4px;
	margin: 3px;
	border: 1px solid #ccc;
}
.tableizer-table th {
	background-color: #104E8B; 
	color: #FFF;
	font-weight: bold;
}
-->
</body></html> 

