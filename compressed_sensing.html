
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Research: Compressed Sensing</title>

<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Fintan Hegarty</title>
	<link href="Style.css" rel="stylesheet" media="screen" />
	<script type="text/JavaScript" src="curvycorners.js"></script>
</head>

<body>
<div id="container">
<div id="wrapper">
<div id="banner">
<h1>Fintan Hegarty</h1>
<p>Research: Compressed Sensing<br />
<p><br />
</p>
</div>
<div id="nav">
<ul>
<li><a href="index.html"><big>Home</big></a></li>
<li><a href="research.html"><big>Research</big></a></li>
<li><a href="teaching.html"><big>Teaching</big></a></li>
<!--<li><a href="stop.html"><big> Eile </big></a></li>-->
<!--<li><a href="index.html"><big>English version</big></a></li>-->
</ul>
</div>   
<div id="page">
<div id="content">

The central problems in compressed sensing can be framed in terms of linear algebra. In
this model, a signal is a vector <i>v</i> in some high-dimensional vector space, <i>R<sup>N</sup></i>. The sampling
process can be described as multiplication by a specially chosen <i>n * N</i> matrix <i>&#934;</i>, called the sensing
matrix. Typically we will have <i>n << N</i>, so that the problem of recovering <i>v</i> from <i>&#934;v</i> is massively
under-determined.<br>
However, for <i>k</i>-sparse vectors <i>v</i> (ie. <i>v</i> contains at most <i>k</i> non-zero entries) it turns out that we can recover
<i>y=&#934;v<br></i> if <i>&#934;</i> has certain properties, as detailed by Candés, Donoho, and Tao.<br>

<br><b>Definition:</b> If &#934; and &#934;' are matrices such that &#934;'<sub>i,j</sub>=&#934;<sub>i,j</sub> for every non-zero entry
of &#934;', then we say that &#934;' is a <i>sparsification</i> of &#934;.<br>
The density of &#934;, &#948;(&#934;) is the proportion of non-zero entries which &#934; contains.<br> 
<br>
In my paper with Ó Catháin and Zhao, we analyse the effect of the sparsification of the sensing matrix &#934; on its recovery capabilities.
<br>
The graph below is similar to the motivational figure from our paper. It shows that the sparsified matrices (&delta;=0.05 and 0.1) outperform the original matrix (&delta;=1). <br>

<img src="https://cloud.githubusercontent.com/assets/4904401/6910681/45d76454-d74e-11e4-92a7-4b574759d38c.png" height="300"/><br>

The Matlab files used to generate the relevant data and these graphs can be found below, along with further information on
parameters etc.
<br><br><br>

<h3>Algorithms</h3>
We considered three different algorithms in the paper - the standard linear programming solver in Matlab, <a href="http://uk.mathworks.com/help/optim/ug/linprog.html">linprog</a>,; 
Orthogonal Matching Pursuit (OMP) - see <a href="http://users.cms.caltech.edu/~jtropp/papers/TG07-Signal-Recovery.pdf">Tropp and Gilbert's paper</a>,
and <a href="http://arxiv.org/pdf/0803.2392v2.pdf">Needell and Tropp's</a> CoSaMP.
<br>
Both OMP and CoSaMP take a priori knowledge of the sparsity of the vector which is to be recovered, and are significantly faster
than linprog. All three, however, exhibit some degree of improvement in recovery for sparsified sensing matrices.

<br><br><br>
<h3>Speed</h3>

In the two graphs below, we examine the effect of sparsification on the run-time of the algorithm. For a range of vector sparsities,
we timed 100 recovery attempts from 200x2000 sensing matrices, using both the standard linear programming solver in Matlab, and the 
specialised CoSaMP algorithm developed by Needell and Tropp.
We see that CoSaMP is significantly faster than the linear programming solver, and that the sparsified matrices recover noticeably
more vectors than the unsparsified ones. The unsparsified CoSaMP is slightly faster in some places - accounted for by the fraction
of a second it takes to sparsify a matrix, but the enormous improvement in recovery shows that sparsification is worthwhile.<br>
These timing computations were run on an Intel(R) Core(TM) i5-3570 @ 3.4GHz with 8GB of RAM.

<img border="”"-100" src="https://cloud.githubusercontent.com/assets/4904401/7548132/17c52d4c-f5f5-11e4-882b-24c5c335aa86.png" width="900" />
<img border="-100" src="https://cloud.githubusercontent.com/assets/4904401/7548227/396592a8-f5f9-11e4-8b0a-0435198aaa9b.png" width="900" />

<br><br><br>

<h3>Relevant Matlab Files</h3>

The main algorithms we used to generate the data for our paper are contained in the following Matlab files:

<li><a href="randomKSparseVector.m">randomKSparseVector</a></li> This algorithm generates the k-sparse vectors used in our
simulations. For clarity, in all experiments used to generate data for the paper, the non-zero entries are drawn from a
uniform distribution on the open interval. However, varying the distribution from which these entries are drawn still produces
data which bolsters the case for sparsification of the sensing matrices.<br><br>

<li><a href="generateMat.m">generateMat</a></li> This algorithm is used to generate our original matrices - the unsparsified
versions - from arguments corresponding to size and the distribution from which the entries were drawn. All matrices, and
vectors, are normalised upon construction.<br><br>

<li><a href="testOneMat.m">testOneMat</a></li> This algorithm takes, as its arguments, the parameters required to generate a
matrix and a vector, and also a list of densities (corresponding to the desired levels of sparsification). A matrix is created,
and then a sparsification thereof for each entry in the list of densities. Then, for each of these sparsified matrices &#934; ,
a random k-sparse vector <i>v</i> is created and a recovery algorithm employed to attempt to recover <i>v</i> from <i>&#934;v</i>
and <i>&#934;</i>. A list of 0s (failures) and 1s (successes) is returned, with one entry corresponding to the recovery attempt
for each sparsification. A recovery is deemed successful if the norm of the estimated vector differs from <i>v</i> by less than
10<sup>-6</sup></i>.<br><br>

<li><a href="sparseCoefficientMat.m">sparseCoefficientMat</a></li> This algorithm creates a matrix of a given size, where
some given proportion of the entries in each column are randomly assigned to have value 1 and the rest are 0.<br><br>

<li><a href="sparsifyMat.m">sparsifyMat</a></li> This algorithm returns a sparsification of the inputted matrix, whose density
is some user-specified value between 0 and 1, created by taking an entrywise product of the inputted matrix with a
sparseCoefficientMat.<br><br>

<li><a href="findRx.m">findRx</a></li> This function finds <i>R<sub>x</sub></i> for a given matrix size and density, ie. the
maximum value <i>k</i> for which we can say with <i>x%</i> certainty that our recovery algorithm will successfully recover a
<i>k</i>-sparse vector. Throughout the paper, we used <i>x=98%</i>.<br><br>

<li><a href="sparsitySuccessTest.m">sparsitySuccessTest</a></li> For each of an inputted range of vector sparsities and matrix
and matrix densities, this test returns the number of successful recoveries over a given number of iterations.<br>

<!--Figure 1 was generated using this <a href="MakingFig1.m">MakingFig1 Matlab file</a> and the data in this <a href="Fig1Data.txt"> Fig1Data .txt file</a>. <br>-->

A figure similar to Figure 1 can be generated using <li><a href="generateSparsitySuccessTestGraph.m">generateSparsitySuccessTestGraph</a></li>.
The parameters used in our example were<br>
<b>generateSparsitySuccessTestGraph(200,2000,'N',[1,0.1,0.05],30,60,500);</b> where
<ul>
<li>200x2000 are the dimensions of the matrices we consider</li>
<li>'N' indicates that the entry values in the matrices are normally distributed (though we then take the absolute value)</li> 
<li>[1,0.1,0.05] are the sparsification levels we test - 1 being the original matrix and 0.05 containing 95% zero entries</li>
<li>30 and 60 are the lower and upper bounds on the sparsity of the signal</li>
<li>500 is the number of iterations over each matrix sparsity level</li>
(Data is sent to sparsitySuccessTestGraphData.txt by default)
</ul>

<!--<table><tbody>
<!--<tr><td><img border=”5″ src="https://cloud.githubusercontent.com/assets/4904401/7548132/17c52d4c-f5f5-11e4-882b-24c5c335aa86.png" height="300" />-->
<!--</td><td><img border=”5″ src="https://cloud.githubusercontent.com/assets/4904401/7548174/4755ca1a-f5f7-11e4-9f9a-4ccc93b69acd.png" height="300" />-->
<!--</td></tr></tbody></table>-->

<!--<img src="https://cloud.githubusercontent.com/assets/4904401/6500414/549da1b0-c303-11e4-979b-6977018992f2.png" height="300"/>
<br>
<style type="text/css">
	table.tableizer-table {
	border: 1px solid #CCC; font-family: Arial, Helvetica, sans-serif;
	font-size: 12px;
} 
.tableizer-table td {
	padding: 4px;
	margin: 3px;
	border: 1px solid #ccc;
}
.tableizer-table th {
	background-color: #104E8B; 
	color: #FFF;
	font-weight: bold;
}
-->
</body></html> 

